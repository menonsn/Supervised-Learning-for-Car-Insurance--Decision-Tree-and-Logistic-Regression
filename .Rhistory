lmodel = lm(Response ~ ., data=bankruptcy.train)
summary(lmodel)
#Check if we can do parsing
library(rpart)
library(rpart.plot)
insurance.largetree <- rpart(formula = Response ~ ., data = bankruptcy.train, cp = 0.001)
par(mfrow=c(1,1))
plotcp(insurance.largetree)
printcp(insurance.largetree)
prune(insurance.largetree, cp = 0.008)
#Subsets
library(leaps)
x <- regsubsets(Response ~ ., data = bankruptcy.train,nvmax=23)
?regsubsets
summary(x)
#Tree with subset = 35 Nodes
tree_new<- ctree(Response~Vehicle.Size+Vehicle.Class+Total.Claim.Amount+Sales.Channel+Renew.Offer.Type+Policy.Type+Number.of.Policies+Number.of.Open.Complaints+Months.Since.Last.Claim+Months.Since.Policy.Inception+Income+Location.Code+Marital.Status+Monthly.Premium.Auto+EmploymentStatus , data = bankruptcy.train, controls = ctree_control(mincriterion = 0.9, minsplit = 200))
tree_new
plot(tree_new)
bankruptcy.rpart <- rpart(formula = Response ~ ., data = bankruptcy.train)
bankruptcy.rpart
prp(bankruptcy.rpart,digits = 4, extra = 1)
?rpart
bankruptcy.rpart1 <- rpart(formula = Response ~ . , data = bankruptcy.train, method = "class", parms = list(loss=matrix(c(0,35,1,0), nrow = 2)))
bankruptcy.rpart1
bankruptcy.rpart <- rpart(formula = Response ~ ., data = bankruptcy.train)
bankruptcy.rpart
prp(bankruptcy.train.rpart,digits = 4, extra = 1)
prp(bankruptcy.rpart,digits = 4, extra = 1)
bankruptcy.rpart0 <- rpart(formula = Response ~ ., data = bankruptcy.train, method = "class")
bankruptcy.rpart0
prp(bankruptcy.rpart0, extra = 1)
pred0<- predict(bankruptcy.rpart0, type="class")
table(bankruptcy.rpart0$Response, pred0, dnn = c("True", "Pred"))
table(bankruptcy.train$Response, pred0, dnn = c("True", "Pred"))
str(bankruptcy.train)
cust<-read.csv("IBM_cust.csv")
str(cust)
rpart0 <- rpart(formula = Response ~ ., data = cust, method = "class")
rpart0
prp(rpart0, extra = 1)
str(cust)
class(cust["State"])
head(cust)
str(cust)
head(cust)
cust_new <- df[ , -which(names(cust) %in% c("Customer","Policy","Effective.To.Date"))]
cust_new <- cust[ , -which(names(cust) %in% c("Customer","Policy","Effective.To.Date"))]
str(cust)
str(cust_new)
head(cust_new)
rpart0 <- rpart(formula = Response ~ ., data = cust_new, method = "class")
rpart0
prp(rpart0, extra = 1)
pred0<- predict(rpart0, type="class")
table(cust_new$Response, pred0, dnn = c("True", "Pred"))
cust_new$State <- as.factor(cust_new$State)
rpart0 <- rpart(formula = Response ~ ., data = cust_new, method = "class")
rpart0
prp(rpart0, extra = 1)
str(cust_new)
pred0<- predict(rpart0, type="class")
table(cust_new$Response, pred0, dnn = c("True", "Pred"))
rpart0
str(cust_new)
?rpart
rpart1 <- rpart(formula = Response ~ ., data = cust_new, cp=0.0001 method = "class")
rpart1 <- rpart(formula = Response ~ ., data = cust_new, cp=0.0001, method = "class")
rpart1
prp(rpart1, extra = 1)
rpart1 <- rpart(formula = Response ~ ., data = cust_new, cp=0.001, method = "class")
rpart1
prp(rpart1, extra = 1)
rpart1 <- rpart(formula = Response ~ ., data = cust_new, cp=0.01, method = "class")
rpart1
prp(rpart1, extra = 1)
rpart1 <- rpart(formula = Response ~ ., data = cust_new, cp=0.001, method = "class")
rpart1
prp(rpart1, extra = 1)
plotcp(rpart1)
printcp(rpart1)
##Libraries for Decision Tree
library(rpart)
library(rpart.plot)
##Creating a copy of the dataset for Modeling the Decision Tree
cus_resp<-read.csv("IBM_cust.csv")
str(cus_resp)
head(cus_resp)
###Removing the columns Customer, Policy and Effective To Date
resp_data <- cus_rep[ , -which(names(cus_rep) %in% c("Customer","Policy","Effective.To.Date"))]
str(resp_data)
head(resp_data)
##Data ready with Categorical and Numeric variables. Start Decision Tree
#Classification Tree with all variables
tree0 <- rpart(formula = Response ~ ., data = resp_data, method = "class")
tree0
prp(tree0, extra = 1)
##Libraries for Decision Tree
library(rpart)
library(rpart.plot)
##Creating a copy of the dataset for Modeling the Decision Tree
cus_resp<-read.csv("IBM_cust.csv")
str(cus_resp)
head(cus_resp)
###Removing the columns Customer, Policy and Effective To Date
resp_data <- cus_rep[ , -which(names(cus_resp) %in% c("Customer","Policy","Effective.To.Date"))]
str(resp_data)
head(resp_data)
##Data ready with Categorical and Numeric variables. Start Decision Tree
#Classification Tree with all variables
tree0 <- rpart(formula = Response ~ ., data = resp_data, method = "class")
tree0
prp(tree0, extra = 1)
##Libraries for Decision Tree
library(rpart)
library(rpart.plot)
##Creating a copy of the dataset for Modeling the Decision Tree
cus_resp<-read.csv("IBM_cust.csv")
str(cus_resp)
head(cus_resp)
###Removing the columns Customer, Policy and Effective To Date
resp_data <- cus_resp[ , -which(names(cus_resp) %in% c("Customer","Policy","Effective.To.Date"))]
str(resp_data)
head(resp_data)
##Data ready with Categorical and Numeric variables. Start Decision Tree
#Classification Tree with all variables
tree0 <- rpart(formula = Response ~ ., data = resp_data, method = "class")
tree0
prp(tree0, extra = 1)
###This tree has 3 leaf nodes with Employement and Renewal as the main split
###Checking predicition rate of tree
pred0<- predict(tree0, type="class")
table(resp_data$Response, pred0, dnn = c("True", "Pred"))
##Libraries for Decision Tree
library(rpart)
library(rpart.plot)
##Creating a copy of the dataset for Modeling the Decision Tree
cus_resp<-read.csv("IBM_cust.csv")
str(cus_resp)
head(cus_resp)
###Removing the columns Customer, Policy and Effective To Date
resp_data <- cus_resp[ , -which(names(cus_resp) %in% c("Customer","Policy","Effective.To.Date"))]
resp_data$Response[resp_data$Response==1] <- 0
resp_data$Response[resp_data$Response==2] <- 1
str(resp_data)
head(resp_data)
##Data ready with Categorical and Numeric variables. Start Decision Tree
#Classification Tree with all variables
tree0 <- rpart(formula = Response ~ ., data = resp_data, method = "class")
tree0
prp(tree0, extra = 1)
#This tree has 3 leaf nodes with Employement and Renewal as the main split
###Checking predicition rate of tree
pred0<- predict(tree0, type="class")
table(resp_data$Response, pred0, dnn = c("True", "Pred"))
#The Tree0 has a prediction rate of
##Libraries for Decision Tree
library(rpart)
library(rpart.plot)
##Creating a copy of the dataset for Modeling the Decision Tree
cus_resp<-read.csv("IBM_cust.csv")
str(cus_resp)
head(cus_resp)
###Removing the columns Customer, Policy and Effective To Date
resp.data <- cus_resp[ , -which(names(cus_resp) %in% c("Customer","Policy","Effective.To.Date"))]
#Convert Response to 1,2 first then convert to 1,0
#resp_data$Response[resp_data$Response==1] <- 0
#resp_data$Response[resp_data$Response==2] <- 1
str(resp.data)
head(resp.data)
#Split the data 70-30
set.seed(13255870)
index <- sample(nrow(resp.data),nrow(resp.data)*0.70)
resp.data.train = resp.data[index,]
resp.data.test = resp.data[-index,]
names(bankruptcy.train)
##Data ready with Categorical and Numeric variables. Start Decision Tree
#Classification Tree with all variables for training dataset
tree0 <- rpart(formula = Response ~ ., data = resp.data.train, method = "class")
tree0
prp(tree0, extra = 1)
#This tree has 3 leaf nodes with Employement and Renewal as the main split
###Checking predicition rate of tree
pred0<- predict(tree0, type="class")
table(resp.data.train$Response, pred0, dnn = c("True", "Pred"))
#The Tree0 has a prediction rate of
#CP = 0.001 (0.01 default)
tree1 <- rpart(formula = Response ~ ., data = resp.data.train, cp=0.001, method = "class")
tree1
prp(tree1, extra = 1)
plotcp(tree1)
printcp(tree1)
rpart1 <- rpart(formula = Response ~ ., data = cust_new, cp=0.0034, method = "class")
rpart1
prp(rpart1, extra
= 1)
plotcp(rpart1)
prp(tree1, extra = 1)
prune(tree, cp = 0.0034)
prp(tree1, extra = 1)
tree_final<-prune(tree, cp = 0.0034)
prp(tree_final, extra = 1)
tree_final<-prune(tree1, cp = 0.0034)
prp(tree_final, extra = 1)
pred_final<- predict(tree_final, type="class")
table(resp.data.train$Response, pred_final, dnn = c("True", "Pred"))
tree1 <- rpart(formula = Response ~ ., data = resp.data.train, cp=0.001, method = "class")
tree1
prp(tree1, extra = 1)
#Depth of the tree is too large and unreadable because of length of variables and their factors.
#Pruning the tree to change Complexity Parameter to reduce depth.
plotcp(tree1)
tree_final<-prune(tree1, cp = 0.0034)
prp(tree_final, extra = 1)
rpart.plot(rpart1, extra=1)
rpart.plot(tree_final, extra=1)
### Tree Performance MSE and MSPE
training.prediction <- predict(tree_final, newdata =resp.data.train)
testing.prediction <- predict(tree_final, newdata =resp.data.test)
train_error <- mean((training.prediction-resp.data.train$Response)^2)
test_error <- mean((testing.prediction-resp.data.train$Response)^2)
### Tree Performance MSE and MSPE
training.prediction <- predict(tree_final, newdata =resp.data.train)
testing.prediction <- predict(tree_final, newdata =resp.data.test)
train_error <- mean((training.prediction-resp.data.train$Response)^2)
test_error <- mean((testing.prediction-resp.data.test$Response)^2)
##Libraries for Decision Tree
library(rpart)
library(rpart.plot)
##Creating a copy of the dataset for Modeling the Decision Tree
cus_resp<-read.csv("IBM_cust.csv")
str(cus_resp)
head(cus_resp)
###Removing the columns Customer, Policy and Effective To Date
resp.data <- cus_resp[ , -which(names(cus_resp) %in% c("Customer","Policy","Effective.To.Date"))]
#Convert Response to 1,2 first then convert to 1,0
#resp_data$Response[resp_data$Response==1] <- 0
#resp_data$Response[resp_data$Response==2] <- 1
str(resp.data)
head(resp.data)
#Split the data 70-30
set.seed(13255870)
index <- sample(nrow(resp.data),nrow(resp.data)*0.70)
resp.data.train = resp.data[index,]
resp.data.test = resp.data[-index,]
names(bankruptcy.train)
##Data ready with Categorical and Numeric variables. Start Decision Tree
#Classification Tree with all variables for training dataset
tree0 <- rpart(formula = Response ~ ., data = resp.data.train, method = "class")
tree0
prp(tree0, extra = 1)
#This tree has 3 leaf nodes with Employement and Renewal as the main split
###Checking predicition rate of tree
pred0<- predict(tree0, type="class")
table(resp.data.train$Response, pred0, dnn = c("True", "Pred"))
#The Tree0 has a prediction rate of 87.5% (5599 out of 6393 predicted right).
###Since only twow variables used, changing Complexity Parameter to add more depth.
#CP = 0.001 (0.01 default)
tree1 <- rpart(formula = Response ~ ., data = resp.data.train, cp=0.001, method = "class")
tree1
prp(tree1, extra = 1)
#Depth of the tree is too large and unreadable because of length of variables and their factors.
#Pruning the tree to change Complexity Parameter to reduce depth.
plotcp(tree1)
printcp(tree1)
#Setting the CP value to 0.0034 where it
tree_final<-prune(tree1, cp = 0.0034)
prp(tree_final, extra = 1)
rpart.plot(tree_final, extra=1)
###Checking predicition rate of final tree
pred_final<- predict(tree_final, type="class")
table(resp.data.train$Response, pred_final, dnn = c("True", "Pred"))
#The prediction rate has increased to 91.7% from 87.5%
### Tree Performance MSE and MSPE
training.prediction <- predict(tree_final, newdata =resp.data.train)
testing.prediction <- predict(tree_final, newdata =resp.data.test)
train_error <- mean((training.prediction-resp.data.train$Response)^2)
test_error <- mean((testing.prediction-resp.data.test$Response)^2)
train_error
test_error
#The prediction rate has increased to 91.7% from 87.5%
?predict
### Tree Performance MSE and MSPE
training.prediction <- predict(tree_final, newdata =resp.data.train, type='class')
train_error <- mean((training.prediction-resp.data.train$Response)^2)
pred.traintree = prediction(pred_final[,2], resp.data.train.train$Response)
perf = performance(pred.traintree, "tpr", "fpr")
plot(perf, colorize=TRUE)
pred.traintree = prediction(pred_final[,2], resp.data.train.train$Response)
perf = performance(pred.traintree, "tpr", "fpr")
plot(perf, colorize=TRUE)
unlist(slot(performance(pred.traintree,"auc"),"y.values"))
pred.traintree = prediction(pred_final, resp.data.train.train$Response)
perf = performance(pred.traintree, "tpr", "fpr")
plot(perf, colorize=TRUE)
unlist(slot(performance(pred.traintree,"auc"),"y.values"))
training.prediction <- predict(tree_final, newdata =resp.data.train, type='class')
testing.prediction <- predict(tree_final, newdata =resp.data.test, type='class')
train_error <- mean((training.prediction-resp.data.train$Response)^2)
test_error <- mean((testing.prediction-resp.data.test$Response)^2)
### Tree Performance MSE and MSPE
training.prediction <- predict(tree_final, newdata =resp.data.train, type='class')
### Tree Performance MSE and MSPE
training.prediction <- predict(tree_final, newdata =resp.data.train, type='class')
training.prediction
pred.traintree = prediction(pred_final, resp.data.train.train$Response)
pred.traintree = prediction(pred_final, resp.data.train.train$Response)
resp.data$Response <- as.numeric(resp.data$Response)
head(resp.data)
resp.data$Response
resp_data$Response[resp_data$Response==1] <- 0
resp_data$Response[resp_data$Response==2] <- 1
] <- 0
resp_data$Response[resp.data$Response==2
resp.data$Response[resp.data$Response==1] <- 0
resp.data$Response[resp.data$Response==2] <- 1
#Convert Response to 1,2 first then convert to 1,0
resp.data$Response <- as.numeric(resp.data$Response)
resp.data$Response[resp.data$Response==1] <- 0
resp.data$Response[resp.data$Response==2] <- 1
resp.data$Response
head(resp.data)
tail(resp.data)
set.seed(13255870)
index <- sample(nrow(resp.data),nrow(resp.data)*0.70)
resp.data.train = resp.data[index,]
resp.data.test = resp.data[-index,]
resp.data.train
unique(resp.data.train)
unique(resp.data.train$Response)
unique(resp.data.test$Response)
View(resp.data.test)
##Libraries for Decision Tree
library(rpart)
library(rpart.plot)
##Creating a copy of the dataset for Modeling the Decision Tree
cus_resp<-read.csv("IBM_cust_final.csv")
str(cus_resp)
head(cus_resp)
###Removing the columns Customer, Policy and Effective To Date
resp.data <- cus_resp[ , -which(names(cus_resp) %in% c("Customer","Policy","Effective.To.Date"))]
set.seed(13255870)
index <- sample(nrow(resp.data),nrow(resp.data)*0.70)
resp.data.train = resp.data[index,]
resp.data.test = resp.data[-index,]
unique(resp.data.test$Response)
names(bankruptcy.train)
tree0 <- rpart(formula = Response ~ ., data = resp.data.train, method = "class")
tree0
prp(tree0, extra = 1)
pred0<- predict(tree0, type="class")
table(resp.data.train$Response, pred0, dnn = c("True", "Pred"))
tree1 <- rpart(formula = Response ~ ., data = resp.data.train, cp=0.001, method = "class")
tree1
prp(tree1, extra = 1)
plotcp(tree1)
printcp(tree1)
#Setting the CP value to 0.0034 where it
tree_final<-prune(tree1, cp = 0.0034)
prp(tree_final, extra = 1)
rpart.plot(tree_final, extra=1)
pred_final<- predict(tree_final, type="class")
table(resp.data.train$Response, pred_final, dnn = c("True", "Pred"))
training.prediction <- predict(tree_final, newdata =resp.data.train, type='class')
testing.prediction <- predict(tree_final, newdata =resp.data.test, type='class')
train_error <- mean((training.prediction-resp.data.train$Response)^2)
test_error <- mean((testing.prediction-resp.data.test$Response)^2)
resp.data.train$Response <- as.numeric(resp.data.train$Response)
resp.data.test$Response <- as.numeric(resp.data.test$Response)
training.prediction <- predict(tree_final, newdata =resp.data.train, type='class')
testing.prediction <- predict(tree_final, newdata =resp.data.test, type='class')
train_error <- mean((training.prediction-resp.data.train$Response)^2)
test_error <- mean((testing.prediction-resp.data.test$Response)^2)
##Libraries for Decision Tree
library(rpart)
library(rpart.plot)
##Creating a copy of the dataset for Modeling the Decision Tree
cus_resp<-read.csv("IBM_cust_final.csv")
str(cus_resp)
head(cus_resp)
###Removing the columns Customer, Policy and Effective To Date
resp.data <- cus_resp[ , -which(names(cus_resp) %in% c("Customer","Policy","Effective.To.Date"))]
#Convert Response to 1,2 first then convert to 1,0
#resp.data$Response <- as.numeric(resp.data$Response)
#resp.data$Response[resp.data$Response==1] <- 0
#resp.data$Response[resp.data$Response==2] <- 1
#str(resp.data)
#tail(resp.data)
#Split the data 70-30
set.seed(13255870)
index <- sample(nrow(resp.data),nrow(resp.data)*0.70)
resp.data.train = resp.data[index,]
resp.data.test = resp.data[-index,]
names(bankruptcy.train)
resp.data.train$Response <- as.numeric(resp.data.train$Response)
resp.data.test$Response <- as.numeric(resp.data.test$Response)
##Data ready with Categorical and Numeric variables. Start Decision Tree
#Classification Tree with all variables for training dataset
tree0 <- rpart(formula = Response ~ ., data = resp.data.train, method = "class")
tree0
prp(tree0, extra = 1)
#This tree has 3 leaf nodes with Employement and Renewal as the main split
###Checking predicition rate of tree
pred0<- predict(tree0, type="class")
table(resp.data.train$Response, pred0, dnn = c("True", "Pred"))
#The Tree0 has a prediction rate of 87.5% (5599 out of 6393 predicted right).
###Since only twow variables used, changing Complexity Parameter to add more depth.
#CP = 0.001 (0.01 default)
tree1 <- rpart(formula = Response ~ ., data = resp.data.train, cp=0.001, method = "class")
tree1
prp(tree1, extra = 1)
#Depth of the tree is too large and unreadable because of length of variables and their factors.
#Pruning the tree to change Complexity Parameter to reduce depth.
plotcp(tree1)
printcp(tree1)
#Setting the CP value to 0.0034 where it
tree_final<-prune(tree1, cp = 0.0034)
prp(tree_final, extra = 1)
rpart.plot(tree_final, extra=1)
###Checking predicition rate of final tree - In Sample
pred_final<- predict(tree_final, type="class")
table(resp.data.train$Response, pred_final, dnn = c("True", "Pred"))
#The prediction rate has increased to 91.7% from 87.5%
#pred.traintree = prediction(pred_final, resp.data.train.train$Response)
#perf = performance(pred.traintree, "tpr", "fpr")
##plot(perf, colorize=TRUE)
unlist(slot(performance(pred.traintree,"auc"),"y.values"))
resp.data.train$Response <- as.numeric(resp.data.train$Response)
resp.data.test$Response <- as.numeric(resp.data.test$Response)
### Tree Performance MSE and MSPE
training.prediction <- predict(tree_final, newdata =resp.data.train, type='class')
testing.prediction <- predict(tree_final, newdata =resp.data.test, type='class')
train_error <- mean((training.prediction-resp.data.train$Response)^2)
test_error <- mean((testing.prediction-resp.data.test$Response)^2)
train_error
test_error
pred.traintree = prediction(pred_final[,2], resp.data.train.train$Response)
perf = performance(pred.final, "tpr", "fpr")
perf = performance(pred_final, "tpr", "fpr")
pred.traintree = prediction(pred_final[,2], resp.data..train$Response)
pred.traintree = prediction(pred_final[,2], resp.data.train$Response)
MR.treetrain<- mean(resp.data.train$Response!= pred_final)
MR.treetrain
###Checking predicition rate of final tree - Out Sample
pred_final<- predict(tree_final, newdata=resp.data.test, type="class")
table(resp.data.test$Response, pred_final, dnn = c("True", "Pred"))
###Checking predicition rate of final tree - In Sample
pred_final<- predict(tree_final, type="class")
table(resp.data.train$Response, pred_final, dnn = c("True", "Pred"))
#The prediction rate has increased to 91.7% from 87.5%
###Checking predicition rate of final tree - Out Sample
pred_final_test<- predict(tree_final, newdata=resp.data.test, type="class")
table(resp.data.test$Response, pred_final_test, dnn = c("True", "Pred"))
#pred.traintree = prediction(pred_final[,2], resp.data.train$Response)
#perf = performance(pred.traintree, "tpr", "fpr")
##plot(perf, colorize=TRUE)
#unlist(slot(performance(pred.traintree,"auc"),"y.values"))
MR.treetrain<- mean(resp.data.train$Response!= pred_final)
MR.treetrain
MR.treetrain<- mean(resp.data.test$Response!= pred_final_test)
MR.treetrain
pred_final
dim(pred_final)
class(pred_final)
typeof(pred_final)
pred.traintree = prediction(pred_final, resp.data.train$Response)
pred.traintree = prediction(pred_final_test, resp.data.train$Response)
###Checking predicition rate of final tree - Out Sample
pred_final_test<- predict(tree_final, newdata=resp.data.test, type="class")
pred.traintree = prediction(pred_final_test, resp.data.train$Response)
pred.traintree = prediction(as.vector(pred_final), resp.data.train$Response)
pred.traintree = prediction(as.vector(pred_final), as.vector(resp.data.train$Response))
pred_final
###Checking predicition rate of final tree - In Sample
pred_final<- predict(tree_final)
pred_final
###Checking predicition rate of final tree - In Sample
pred_final<- predict(tree_final, type="class")
pred_final
pred.traintree = prediction(pred_final, resp.data.train$Response)
###Checking predicition rate of final tree - In Sample
pred_final<- predict(tree_final, type="response")
###Checking predicition rate of final tree - In Sample
pred_final<- predict(tree_final, type="response")
###Checking predicition rate of final tree - In Sample
pred_final<- predict(tree_final, type="class")
pred_final<- predict(tree_final, type="response")
pred_final<- predict(tree_final, type="prob")
pred.traintree = prediction(pred_final, resp.data.train$Response)
pred_final<- predict(tree_final, type="class")
pred_final
table(resp.data.train$Response, pred_final, dnn = c("True", "Pred"))
pred.traintree = prediction(pred_final, resp.data.train$Response)
typeof(pred_final)
tree_final
tree0
tree1
typeof(resp.data.train$Response)
pred.traintree = prediction(as.double(pred_final), resp.data.train$Response)
perf = performance(pred.traintree, "tpr", "fpr")
plot(perf, colorize=TRUE)
unlist(slot(performance(pred.traintree,"auc"),"y.values"))
pred.traintree.test = prediction(as.double(pred_final_test), resp.data.test$Response)
perf = performance(pred.traintree.test, "tpr", "fpr")
plot(perf, colorize=TRUE)
unlist(slot(performance(pred.traintree.test,"auc"),"y.values"))
#Mean error for training data
MR.treetrain<- mean(resp.data.train$Response!= pred_final)
MR.treetrain
#Mean error fro testing data
MR.treetrain<- mean(resp.data.test$Response!= pred_final_test)
MR.treetrain
